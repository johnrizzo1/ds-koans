{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - Data Science Koans\n",
    "\n",
    "Welcome to Notebook 11: Dimensionality Reduction!\n",
    "\n",
    "## What You Will Learn\n",
    "- Principal Component Analysis (PCA) fundamentals\n",
    "- Choosing optimal number of components\n",
    "- Interpreting explained variance and feature loadings\n",
    "- t-SNE for non-linear visualization\n",
    "- Understanding the curse of dimensionality\n",
    "- When and how to apply dimensionality reduction\n",
    "\n",
    "## Why This Matters\n",
    "As datasets grow larger and more complex, dimensionality reduction becomes crucial for:\n",
    "- **Visualization**: Project high-D data to 2D/3D for human understanding\n",
    "- **Performance**: Reduce computational cost and memory usage\n",
    "- **Noise Reduction**: Remove irrelevant features and focus on signal\n",
    "- **Storage**: Compress data while preserving important patterns\n",
    "- **Analysis**: Overcome the curse of dimensionality\n",
    "\n",
    "## Prerequisites\n",
    "- Clustering basics (Notebook 10)\n",
    "- Linear algebra foundations from Notebook 01 (complete KOANs 1.11-1.24)\n",
    "- Differential calculus from Notebook 16 (focus on KOANs 16.1-16.22)\n",
    "- Experience with scikit-learn\n",
    "\n",
    "## How to Use\n",
    "1. Read each koan's objective and explanation\n",
    "2. Complete the TODO sections with your implementation\n",
    "3. Run the validation to check correctness\n",
    "4. Learn from any errors and iterate\n",
    "5. Move to next koan when passing\n",
    "\n",
    "Let's reduce some dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run first!\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits, load_iris, load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "\n",
    "validator = KoanValidator(\"11_dimensionality_reduction\")\n",
    "tracker = ProgressTracker()\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current progress: {tracker.get_notebook_progress('11_dimensionality_reduction')}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.1: Principal Component Analysis\n",
    "**Objective**: Reduce Iris dataset to 2D using PCA  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "PCA finds the directions (principal components) of maximum variance in your data. \n",
    "The first principal component captures the most variance, the second captures the most remaining variance orthogonal to the first, and so on.\n",
    "\n",
    "**Key Concept**: PCA is a linear transformation that projects data onto a lower-dimensional subspace while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca_to_iris():\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce the Iris dataset from 4D to 2D.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load the iris dataset\n",
    "    2. Get the feature matrix (iris.data)\n",
    "    3. Create PCA with 2 components\n",
    "    4. Fit and transform the data\n",
    "    5. Return the 2D transformed data\n",
    "    \"\"\"\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    \n",
    "    # TODO: Create PCA instance with n_components=2\n",
    "    pca = None\n",
    "    \n",
    "    # TODO: Fit PCA and transform data to 2D\n",
    "    X_2d = None\n",
    "    \n",
    "    return X_2d\n",
    "\n",
    "@validator.koan(1, \"Principal Component Analysis\", difficulty=\"Intermediate-Advanced\")\n",
    "def validate():\n",
    "    result = apply_pca_to_iris()\n",
    "    \n",
    "    # Verify we got the right shape\n",
    "    assert result is not None, \"Function returned None - did you forget to implement it?\"\n",
    "    assert isinstance(result, np.ndarray), \"Result should be a numpy array\"\n",
    "    assert result.shape == (150, 2), f\"Expected shape (150, 2), got {result.shape}\"\n",
    "    \n",
    "    # Verify it's actually a meaningful transformation\n",
    "    assert not np.allclose(result, 0), \"Result is all zeros - check your PCA implementation\"\n",
    "    \n",
    "    print(\"\u2713 Successfully reduced Iris from 4D to 2D!\")\n",
    "    print(f\"  - Original shape: (150, 4)\")\n",
    "    print(f\"  - Reduced shape: {result.shape}\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.2: Explained Variance\n",
    "**Objective**: Calculate cumulative explained variance ratios  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "When we reduce dimensions, we lose some information. The explained variance ratio tells us how much of the original variance each component captures.\n",
    "\n",
    "**Key Concept**: `explained_variance_ratio_` shows the percentage of total dataset variance explained by each principal component. This helps us understand how much information we retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_digits_variance():\n",
    "    \"\"\"\n",
    "    Apply PCA to digits dataset and return cumulative explained variance.\n",
    "    \n",
    "    The digits dataset has 64 features (8x8 pixel images).\n",
    "    We'll see how much variance the first 10 components capture.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Cumulative explained variance ratios for first 10 components\n",
    "    \"\"\"\n",
    "    digits = load_digits()\n",
    "    X = digits.data  # Shape: (1797, 64)\n",
    "    \n",
    "    # TODO: Create PCA with 10 components and fit to the digits data\n",
    "    pca = None\n",
    "    \n",
    "    # TODO: Get explained variance ratios and compute cumulative sum\n",
    "    # Hint: Use np.cumsum() on pca.explained_variance_ratio_\n",
    "    cumulative_variance = None\n",
    "    \n",
    "    return cumulative_variance\n",
    "\n",
    "@validator.koan(2, \"Explained Variance\", difficulty=\"Intermediate-Advanced\") \n",
    "def validate():\n",
    "    result = analyze_digits_variance()\n",
    "    \n",
    "    assert result is not None, \"Function returned None\"\n",
    "    assert isinstance(result, np.ndarray), \"Result should be a numpy array\"\n",
    "    assert len(result) == 10, f\"Expected 10 values, got {len(result)}\"\n",
    "    \n",
    "    # Check that it's cumulative (monotonically increasing)\n",
    "    assert np.all(np.diff(result) >= 0), \"Cumulative variance should be increasing\"\n",
    "    \n",
    "    # Should be between 0 and 1\n",
    "    assert np.all(result >= 0) and np.all(result <= 1), \"Variance ratios should be between 0 and 1\"\n",
    "    \n",
    "    # First 10 components should explain significant variance\n",
    "    assert result[-1] > 0.5, f\"First 10 components should explain >50% variance, got {result[-1]*100:.1f}%\"\n",
    "    \n",
    "    print(f\"\u2713 First 10 components explain {result[-1]*100:.1f}% of variance\")\n",
    "    print(f\"  - 1st component: {result[0]*100:.1f}%\")\n",
    "    print(f\"  - 2nd component: {result[1]*100:.1f}%\") \n",
    "    print(f\"  - 10th component: {result[-1]*100:.1f}%\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.3: Scree Plot and Component Selection\n",
    "**Objective**: Determine optimal number of components for 95% variance  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "A common question in PCA is: \"How many components should I keep?\" One approach is to choose enough components to retain a specific percentage (like 95%) of the original variance.\n",
    "\n",
    "**Key Concept**: The \"elbow method\" looks for the point where adding more components gives diminishing returns in explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_components_for_variance_threshold():\n",
    "    \"\"\"\n",
    "    Find minimum number of components needed to explain 95% of variance.\n",
    "    \n",
    "    We'll use more components (up to 50) to find the exact cutoff.\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of components needed for 95% explained variance\n",
    "    \"\"\"\n",
    "    digits = load_digits()\n",
    "    X = digits.data\n",
    "    \n",
    "    # TODO: Create PCA with 50 components and fit to digits data\n",
    "    pca = None\n",
    "    \n",
    "    # TODO: Calculate cumulative explained variance\n",
    "    cumulative_variance = None\n",
    "    \n",
    "    # TODO: Find first index where cumulative variance >= 0.95\n",
    "    # Hint: Use np.argmax(cumulative_variance >= 0.95) and add 1\n",
    "    n_components_95 = None\n",
    "    \n",
    "    return n_components_95\n",
    "\n",
    "@validator.koan(3, \"Scree Plot and Component Selection\", difficulty=\"Intermediate-Advanced\")\n",
    "def validate():\n",
    "    result = find_components_for_variance_threshold()\n",
    "    \n",
    "    assert result is not None, \"Function returned None\"\n",
    "    assert isinstance(result, (int, np.integer)), \"Result should be an integer\"\n",
    "    assert 1 <= result <= 50, f\"Expected 1-50 components, got {result}\"\n",
    "    \n",
    "    # Verify the result by checking it actually gives >= 95% variance\n",
    "    digits = load_digits()\n",
    "    pca_check = PCA(n_components=result).fit(digits.data)\n",
    "    variance_check = np.sum(pca_check.explained_variance_ratio_)\n",
    "    assert variance_check >= 0.95, f\"Components {result} only explain {variance_check*100:.1f}% variance\"\n",
    "    \n",
    "    print(f\"\u2713 Need {result} components to explain \u226595% variance\")\n",
    "    print(f\"  - Actual variance explained: {variance_check*100:.2f}%\")\n",
    "    print(f\"  - Dimension reduction: 64 \u2192 {result} ({(1-result/64)*100:.1f}% reduction)\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.4: PCA for Visualization  \n",
    "**Objective**: Project digits dataset to 2D for visualization  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "One of the most powerful uses of PCA is visualizing high-dimensional data. By projecting to 2D or 3D, we can plot data and look for patterns, clusters, or outliers.\n",
    "\n",
    "**Key Concept**: While we lose information going to 2D, we gain the ability to visualize and intuitively understand our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_digits_for_visualization():\n",
    "    \"\"\"\n",
    "    Project the digits dataset to 2D for visualization purposes.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_2d, y) where X_2d is the 2D projection and y are the digit labels\n",
    "    \"\"\"\n",
    "    digits = load_digits()\n",
    "    X = digits.data    # Shape: (1797, 64) - flattened 8x8 images\n",
    "    y = digits.target  # Shape: (1797,) - digit labels 0-9\n",
    "    \n",
    "    # TODO: Create PCA with 2 components\n",
    "    pca = None\n",
    "    \n",
    "    # TODO: Fit PCA and transform data to 2D\n",
    "    X_2d = None\n",
    "    \n",
    "    return X_2d, y\n",
    "\n",
    "@validator.koan(4, \"PCA for Visualization\", difficulty=\"Intermediate-Advanced\")\n",
    "def validate():\n",
    "    X_2d, y = project_digits_for_visualization()\n",
    "    \n",
    "    assert X_2d is not None and y is not None, \"Function returned None values\"\n",
    "    assert isinstance(X_2d, np.ndarray), \"X_2d should be numpy array\"\n",
    "    assert isinstance(y, np.ndarray), \"y should be numpy array\"\n",
    "    assert X_2d.shape == (1797, 2), f\"Expected X_2d shape (1797, 2), got {X_2d.shape}\"\n",
    "    assert len(y) == 1797, f\"Expected y length 1797, got {len(y)}\"\n",
    "    assert set(np.unique(y)) == set(range(10)), \"y should contain digits 0-9\"\n",
    "    \n",
    "    # Check that the projection has reasonable spread\n",
    "    assert np.std(X_2d[:, 0]) > 1, \"First component should have reasonable variance\"\n",
    "    assert np.std(X_2d[:, 1]) > 1, \"Second component should have reasonable variance\"\n",
    "    \n",
    "    print(\"\u2713 Successfully projected 64D digits to 2D!\")\n",
    "    print(f\"  - Original: {digits.data.shape}\")\n",
    "    print(f\"  - Projected: {X_2d.shape}\")\n",
    "    print(f\"  - PC1 range: [{X_2d[:, 0].min():.1f}, {X_2d[:, 0].max():.1f}]\")\n",
    "    print(f\"  - PC2 range: [{X_2d[:, 1].min():.1f}, {X_2d[:, 1].max():.1f}]\")\n",
    "    \n",
    "    # Calculate variance explained by first 2 components\n",
    "    digits = load_digits()\n",
    "    pca_check = PCA(n_components=2).fit(digits.data)\n",
    "    variance_2d = np.sum(pca_check.explained_variance_ratio_)\n",
    "    print(f\"  - Variance retained: {variance_2d*100:.1f}%\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.5: Feature Loadings\n",
    "**Objective**: Interpret PCA components through feature loadings  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "Feature loadings tell us which original features contribute most to each principal component. This helps us understand what each PC represents in terms of the original variables.\n",
    "\n",
    "**Key Concept**: `pca.components_` contains the loadings. Each row is a PC, each column is an original feature. Large absolute values indicate high importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_pca_loadings():\n",
    "    \"\"\"\n",
    "    Fit PCA to Iris dataset and return the feature loadings (components).\n",
    "    \n",
    "    Iris features are: sepal_length, sepal_width, petal_length, petal_width\n",
    "    We'll see which features contribute most to each of the first 2 components.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: PCA components/loadings matrix, shape (2, 4)\n",
    "    \"\"\"\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    \n",
    "    # TODO: Create PCA with 2 components and fit to iris data  \n",
    "    pca = None\n",
    "    \n",
    "    # TODO: Return the components (loadings) matrix\n",
    "    # Hint: Use pca.components_\n",
    "    loadings = None\n",
    "    \n",
    "    return loadings\n",
    "\n",
    "@validator.koan(5, \"Feature Loadings\", difficulty=\"Intermediate-Advanced\")\n",
    "def validate():\n",
    "    result = get_iris_pca_loadings()\n",
    "    \n",
    "    assert result is not None, \"Function returned None\"\n",
    "    assert isinstance(result, np.ndarray), \"Result should be numpy array\"\n",
    "    assert result.shape == (2, 4), f\"Expected shape (2, 4), got {result.shape}\"\n",
    "    \n",
    "    # Check that loadings are unit vectors (approximately)\n",
    "    pc1_norm = np.linalg.norm(result[0])\n",
    "    pc2_norm = np.linalg.norm(result[1])\n",
    "    assert abs(pc1_norm - 1.0) < 0.1, f\"PC1 should be unit vector, norm = {pc1_norm:.3f}\"\n",
    "    assert abs(pc2_norm - 1.0) < 0.1, f\"PC2 should be unit vector, norm = {pc2_norm:.3f}\"\n",
    "    \n",
    "    # Check that PCs are orthogonal\n",
    "    dot_product = np.dot(result[0], result[1])\n",
    "    assert abs(dot_product) < 0.1, f\"PCs should be orthogonal, dot product = {dot_product:.3f}\"\n",
    "    \n",
    "    print(\"\u2713 Successfully extracted PCA loadings!\")\n",
    "    print(f\"  - Shape: {result.shape}\")\n",
    "    print(\"  - PC1 loadings:\", [f\"{x:.3f}\" for x in result[0]])\n",
    "    print(\"  - PC2 loadings:\", [f\"{x:.3f}\" for x in result[1]])\n",
    "    \n",
    "    # Interpret the loadings\n",
    "    feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    print(\"\\n  Feature importance in PC1:\")\n",
    "    for i, name in enumerate(feature_names):\n",
    "        print(f\"    {name}: {result[0, i]:.3f}\")\n",
    "    \n",
    "    print(\"\\n  Feature importance in PC2:\")\n",
    "    for i, name in enumerate(feature_names):\n",
    "        print(f\"    {name}: {result[1, i]:.3f}\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.6: t-SNE for Non-linear Visualization\n",
    "**Objective**: Apply t-SNE for non-linear dimensionality reduction  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "While PCA is linear, t-SNE (t-distributed Stochastic Neighbor Embedding) can capture non-linear relationships. It's especially good for visualization, preserving local neighborhoods.\n",
    "\n",
    "**Key Concept**: t-SNE is primarily for visualization, not feature reduction. It's computationally expensive but often reveals structure that PCA misses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tsne_to_digits():\n",
    "    \"\"\"\n",
    "    Apply t-SNE to a subset of the digits dataset for visualization.\n",
    "    \n",
    "    Note: We'll use only the first 500 samples to keep computation time reasonable.\n",
    "    t-SNE is much slower than PCA, especially on large datasets.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 2D t-SNE embedding, shape (500, 2)\n",
    "    \"\"\"\n",
    "    digits = load_digits()\n",
    "    \n",
    "    # Use only first 500 samples for reasonable computation time\n",
    "    X_subset = digits.data[:500]\n",
    "    \n",
    "    # TODO: Create TSNE instance with n_components=2 and random_state=42\n",
    "    # Hint: Use TSNE(n_components=2, random_state=42)\n",
    "    tsne = None\n",
    "    \n",
    "    # TODO: Fit and transform the data subset\n",
    "    X_tsne = None\n",
    "    \n",
    "    return X_tsne\n",
    "\n",
    "@validator.koan(6, \"t-SNE for Non-linear Visualization\", difficulty=\"Intermediate-Advanced\")\n",
    "def validate():\n",
    "    result = apply_tsne_to_digits()\n",
    "    \n",
    "    assert result is not None, \"Function returned None\"\n",
    "    assert isinstance(result, np.ndarray), \"Result should be numpy array\"\n",
    "    assert result.shape == (500, 2), f\"Expected shape (500, 2), got {result.shape}\"\n",
    "    \n",
    "    # Check that embedding has reasonable spread\n",
    "    assert np.std(result[:, 0]) > 1, \"t-SNE dimension 1 should have reasonable variance\"\n",
    "    assert np.std(result[:, 1]) > 1, \"t-SNE dimension 2 should have reasonable variance\"\n",
    "    \n",
    "    # Check that values are not all the same\n",
    "    assert len(np.unique(result[:, 0])) > 10, \"t-SNE should produce diverse values\"\n",
    "    assert len(np.unique(result[:, 1])) > 10, \"t-SNE should produce diverse values\"\n",
    "    \n",
    "    print(\"\u2713 Successfully applied t-SNE to digits dataset!\")\n",
    "    print(f\"  - Input shape: (500, 64)\")\n",
    "    print(f\"  - Output shape: {result.shape}\")\n",
    "    print(f\"  - Dimension 1 range: [{result[:, 0].min():.1f}, {result[:, 0].max():.1f}]\")\n",
    "    print(f\"  - Dimension 2 range: [{result[:, 1].min():.1f}, {result[:, 1].max():.1f}]\")\n",
    "    \n",
    "    print(\"\\n  \ud83d\udca1 Tip: t-SNE is great for visualization but:\")\n",
    "    print(\"     \u2022 Much slower than PCA\")\n",
    "    print(\"     \u2022 Not deterministic (despite random_state)\")\n",
    "    print(\"     \u2022 Distances between clusters are not meaningful\")\n",
    "    print(\"     \u2022 Primarily for visualization, not preprocessing\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.7: Data Standardization Before PCA\n",
    "**Objective**: Understand the importance of scaling features before PCA  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "PCA is sensitive to feature scales. If one feature has much larger values than others, it will dominate the principal components. Standardization ensures each feature contributes equally.\n",
    "\n",
    "**Key Concept**: Always standardize features before PCA when they have different units or scales. Use StandardScaler to make features have mean=0 and std=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_breast_cancer_data():\n",
    "    \"\"\"\n",
    "    Load breast cancer dataset and standardize the features.\n",
    "    \n",
    "    The breast cancer dataset has 30 features with very different scales.\n",
    "    Some are tiny (like concavity) while others are large (like area).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Standardized feature matrix where each feature has mean\u22480, std\u22481\n",
    "    \"\"\"\n",
    "    # Load the breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X = cancer.data  # Shape: (569, 30)\n",
    "    \n",
    "    # TODO: Create StandardScaler instance\n",
    "    scaler = None\n",
    "    \n",
    "    # TODO: Fit the scaler and transform the data\n",
    "    # Hint: Use scaler.fit_transform(X)\n",
    "    X_standardized = None\n",
    "    \n",
    "    return X_standardized\n",
    "\n",
    "@validator.koan(7, \"Data Standardization Before PCA\", difficulty=\"Intermediate-Advanced\")\n",
    "def validate():\n",
    "    result = standardize_breast_cancer_data()\n",
    "    \n",
    "    assert result is not None, \"Function returned None\"\n",
    "    assert isinstance(result, np.ndarray), \"Result should be numpy array\"\n",
    "    assert result.shape == (569, 30), f\"Expected shape (569, 30), got {result.shape}\"\n",
    "    \n",
    "    # Check standardization: mean should be close to 0, std close to 1\n",
    "    means = np.mean(result, axis=0)\n",
    "    stds = np.std(result, axis=0, ddof=0)\n",
    "    \n",
    "    assert np.allclose(means, 0, atol=1e-10), f\"Means should be ~0, got max={np.max(np.abs(means)):.2e}\"\n",
    "    assert np.allclose(stds, 1, atol=1e-10), f\"Stds should be ~1, got range=[{np.min(stds):.3f}, {np.max(stds):.3f}]\"\n",
    "    \n",
    "    print(\"\u2713 Successfully standardized breast cancer dataset!\")\n",
    "    print(f\"  - Shape: {result.shape}\")\n",
    "    print(f\"  - Mean range: [{np.min(means):.2e}, {np.max(means):.2e}]\")\n",
    "    print(f\"  - Std range: [{np.min(stds):.3f}, {np.max(stds):.3f}]\")\n",
    "    \n",
    "    # Show the impact of standardization\n",
    "    cancer = load_breast_cancer()\n",
    "    original_means = np.mean(cancer.data, axis=0)\n",
    "    original_stds = np.std(cancer.data, axis=0, ddof=0)\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udcca Before standardization:\")\n",
    "    print(f\"     Mean range: [{np.min(original_means):.1f}, {np.max(original_means):.1f}]\")  \n",
    "    print(f\"     Std range: [{np.min(original_stds):.1f}, {np.max(original_stds):.1f}]\")\n",
    "    print(f\"  \ud83d\udcca After standardization:\")\n",
    "    print(f\"     Mean range: [{np.min(means):.2e}, {np.max(means):.2e}]\")\n",
    "    print(f\"     Std range: [{np.min(stds):.3f}, {np.max(stds):.3f}]\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 11.8: The Curse of Dimensionality\n",
    "**Objective**: Demonstrate how distance metrics behave in high dimensions  \n",
    "**Difficulty**: Intermediate-Advanced\n",
    "\n",
    "As dimensions increase, several strange things happen:\n",
    "- All points become approximately equidistant\n",
    "- The difference between nearest and farthest neighbors shrinks\n",
    "- Most of the volume of a hypersphere is near its surface\n",
    "\n",
    "**Key Concept**: In high dimensions, intuitions from 2D/3D often fail. This is why dimensionality reduction is crucial for many ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_curse_of_dimensionality():\n",
    "    \"\"\"\n",
    "    Compare distance distributions in low vs high dimensions.\n",
    "    \n",
    "    We'll generate random points in 2D vs 100D and compare how the \n",
    "    pairwise distances are distributed. In high dimensions, all distances\n",
    "    become more similar (less variance).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (std_high_d, std_low_d) - standard deviations of distances\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    # Generate random points in different dimensions\n",
    "    n_points = 100\n",
    "    \n",
    "    # TODO: Generate 100 random points in 100 dimensions (shape: 100 x 100)\n",
    "    # Hint: Use np.random.rand(n_points, 100)\n",
    "    points_high_d = None\n",
    "    \n",
    "    # TODO: Generate 100 random points in 2 dimensions (shape: 100 x 2)  \n",
    "    points_low_d = None\n",
    "    \n",
    "    # TODO: Calculate pairwise distances for both using pdist\n",
    "    # Hint: from scipy.spatial.distance import pdist (already imported)\n",
    "    distances_high_d = None\n",
    "    distances_low_d = None\n",
    "    \n",
    "    # TODO: Calculate standard deviation of distances for both\n",
    "    std_high_d = None\n",
    "    std_low_d = None\n",
    "    \n",
    "    return std_high_d, std_low_d\n",
    "\n",
    "@validator.koan(8, \"The Curse of Dimensionality\", difficulty=\"Intermediate-Advanced\") \n",
    "def validate():\n",
    "    std_high_d, std_low_d = demonstrate_curse_of_dimensionality()\n",
    "    \n",
    "    assert std_high_d is not None and std_low_d is not None, \"Function returned None values\"\n",
    "    assert isinstance(std_high_d, (float, np.floating)), \"std_high_d should be a number\"\n",
    "    assert isinstance(std_low_d, (float, np.floating)), \"std_low_d should be a number\"\n",
    "    \n",
    "    # In high dimensions, distances become more uniform (lower std)\n",
    "    assert std_high_d < std_low_d, f\"High-D std ({std_high_d:.3f}) should be < Low-D std ({std_low_d:.3f})\"\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert std_high_d > 0, \"Standard deviation should be positive\"\n",
    "    assert std_low_d > 0, \"Standard deviation should be positive\"\n",
    "    assert std_high_d < 1.0, \"High-D std seems too large\"\n",
    "    assert std_low_d < 1.0, \"Low-D std seems too large\"\n",
    "    \n",
    "    print(\"\u2713 Successfully demonstrated curse of dimensionality!\")\n",
    "    print(f\"  - 100D distance std: {std_high_d:.4f}\")\n",
    "    print(f\"  - 2D distance std:   {std_low_d:.4f}\")\n",
    "    print(f\"  - Ratio (100D/2D):   {std_high_d/std_low_d:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83c\udfaf Key insight: In high dimensions, all points become\")\n",
    "    print(f\"     approximately equidistant! This makes many ML algorithms\")\n",
    "    print(f\"     struggle because distance-based metrics become less informative.\")\n",
    "    \n",
    "    # Show actual distance distributions\n",
    "    np.random.seed(42)\n",
    "    points_high = np.random.rand(100, 100)\n",
    "    points_low = np.random.rand(100, 2)\n",
    "    dist_high = pdist(points_high)\n",
    "    dist_low = pdist(points_low)\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udcca Distance statistics:\")\n",
    "    print(f\"     100D: mean={np.mean(dist_high):.3f}, std={np.std(dist_high):.3f}\")\n",
    "    print(f\"     2D:   mean={np.mean(dist_low):.3f}, std={np.std(dist_low):.3f}\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n",
    "\n",
    "You have mastered the fundamentals of dimensionality reduction! \n",
    "\n",
    "### What You've Learned\n",
    "- \u2705 **PCA Fundamentals**: Applied linear dimensionality reduction\n",
    "- \u2705 **Variance Analysis**: Understood explained variance ratios and component selection  \n",
    "- \u2705 **Feature Interpretation**: Analyzed PCA loadings to understand components\n",
    "- \u2705 **Visualization**: Used PCA and t-SNE for data visualization\n",
    "- \u2705 **Preprocessing**: Learned the importance of standardization\n",
    "- \u2705 **Theory**: Understood the curse of dimensionality\n",
    "\n",
    "### Key Takeaways\n",
    "1. **PCA** is fast, interpretable, and great for preprocessing\n",
    "2. **t-SNE** excels at visualization but is computationally expensive\n",
    "3. **Always standardize** when features have different scales\n",
    "4. **Explained variance** helps choose the right number of components\n",
    "5. **High dimensions** make distance-based algorithms struggle\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 12**: Ensemble Methods (Random Forests, Gradient Boosting)\n",
    "- **Advanced**: Try UMAP, Autoencoders, or Manifold Learning\n",
    "- **Practice**: Apply dimensionality reduction to your own datasets\n",
    "\n",
    "### Real-World Applications\n",
    "- **Computer Vision**: Reduce image dimensionality for faster processing\n",
    "- **Genomics**: Analyze high-dimensional gene expression data  \n",
    "- **Finance**: Portfolio optimization with many assets\n",
    "- **NLP**: Reduce word embedding dimensions\n",
    "- **Recommendation**: Collaborative filtering with matrix factorization\n",
    "\n",
    "Keep practicing and exploring! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Progress Check\n",
    "progress = tracker.get_notebook_progress('11_dimensionality_reduction')\n",
    "print(f\"\\n\ud83d\udcca Your Progress: {progress}% complete!\")\n",
    "\n",
    "if progress == 100:\n",
    "    print(\"\ud83c\udf89 Perfect! You've mastered all dimensionality reduction koans!\")\n",
    "    print(\"\ud83c\udfaf You're ready for Notebook 12: Ensemble Methods\")\n",
    "elif progress >= 75:\n",
    "    print(\"\ud83c\udf1f Excellent progress! Just a few more koans to go.\")\n",
    "elif progress >= 50:\n",
    "    print(\"\ud83d\udcaa Great work! You're halfway through the challenges.\")\n",
    "else:\n",
    "    print(\"\ud83d\ude80 Keep going! Each koan builds important skills.\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Overall course progress:\")\n",
    "total_notebooks = 15\n",
    "completed_notebooks = len([nb for nb in range(1, 12) if tracker.get_notebook_progress(f'{nb:02d}_*') == 100])\n",
    "print(f\"   Completed notebooks: {completed_notebooks}/{total_notebooks}\")\n",
    "print(f\"   Course progress: {(completed_notebooks/total_notebooks)*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}