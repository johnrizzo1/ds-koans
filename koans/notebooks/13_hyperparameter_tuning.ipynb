{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Data Science Koans\n",
    "\n",
    "Welcome to Notebook 13: Hyperparameter Tuning!\n",
    "\n",
    "## What You Will Learn\n",
    "- Grid Search for exhaustive parameter exploration\n",
    "- Random Search for efficient parameter sampling  \n",
    "- Bayesian Optimization for intelligent search\n",
    "- Early Stopping techniques for efficiency\n",
    "- Nested Cross-Validation for unbiased evaluation\n",
    "- AutoML basics for automated model selection\n",
    "\n",
    "## Why This Matters\n",
    "Hyperparameter tuning can dramatically improve model performance:\n",
    "- **Performance Gains**: Often 10-30% accuracy improvement\n",
    "- **Generalization**: Better validation performance through proper tuning\n",
    "- **Efficiency**: Automated search saves time and finds better solutions\n",
    "- **Robustness**: Reduces sensitivity to parameter choices\n",
    "- **Competition Edge**: Essential for winning ML competitions\n",
    "\n",
    "## Key Concepts\n",
    "- **Hyperparameters**: Model configuration settings (not learned from data)\n",
    "- **Search Space**: Range of possible parameter values to explore\n",
    "- **Cross-Validation**: Robust evaluation during parameter search\n",
    "- **Overfitting Risk**: Tuning on test data leads to overoptimistic results\n",
    "- **Computational Trade-offs**: Balancing search thoroughness vs. time\n",
    "\n",
    "## Prerequisites\n",
    "- Ensemble Methods (Notebook 12)\n",
    "- Understanding of cross-validation\n",
    "- Differential calculus from Notebook 16 (gradients & Hessians)\n",
    "- Experience with scikit-learn model evaluation\n",
    "\n",
    "## How to Use\n",
    "1. Understand each search strategy's strengths and weaknesses\n",
    "2. Implement the TODO sections with proper parameter definitions\n",
    "3. Run validations to verify search implementations\n",
    "4. Compare different tuning approaches\n",
    "5. Learn to avoid common pitfalls like data leakage\n",
    "\n",
    "Ready to optimize your models to their full potential? \ud83c\udfaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run first!\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer, load_digits, make_classification\n",
    "from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV, \n",
    "                                     train_test_split, cross_val_score,\n",
    "                                     validation_curve, learning_curve)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint, uniform\n",
    "import time\n",
    "\n",
    "# Optional advanced libraries\n",
    "try:\n",
    "    from sklearn.experimental import enable_halving_search_cv\n",
    "    from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "    HALVING_AVAILABLE = True\n",
    "    print(\"\u2713 Halving search methods available\")\n",
    "except ImportError:\n",
    "    HALVING_AVAILABLE = False\n",
    "    print(\"\u26a0\ufe0f Halving search not available (sklearn < 0.24)\")\n",
    "\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    BAYESIAN_AVAILABLE = True\n",
    "    print(\"\u2713 Bayesian optimization available\")\n",
    "except ImportError:\n",
    "    BAYESIAN_AVAILABLE = False\n",
    "    print(\"\u26a0\ufe0f Bayesian optimization not available (install: pip install scikit-optimize)\")\n",
    "\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "\n",
    "validator = KoanValidator(\"13_hyperparameter_tuning\")\n",
    "tracker = ProgressTracker()\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current progress: {tracker.get_notebook_progress('13_hyperparameter_tuning')}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.1: Grid Search - Exhaustive Parameter Search\n",
    "**Objective**: Use GridSearchCV to find optimal Random Forest parameters  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Grid Search evaluates every combination of specified parameter values. It's thorough but can be computationally expensive with large parameter spaces.\n",
    "\n",
    "**Key Concept**: Grid Search guarantees finding the best combination within your specified parameter grid, but grows exponentially with the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest_with_grid_search():\n",
    "    \"\"\"\n",
    "    Use Grid Search to optimize Random Forest hyperparameters.\n",
    "    \n",
    "    We'll tune: n_estimators, max_depth, min_samples_split, min_samples_leaf\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains best parameters, best score, and search results\n",
    "    \"\"\"\n",
    "    # Load and prepare data\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Define parameter grid for Random Forest\n",
    "    param_grid = {\n",
    "        'n_estimators': None,      # [50, 100, 200]\n",
    "        'max_depth': None,         # [None, 10, 20, 30]  \n",
    "        'min_samples_split': None, # [2, 5, 10]\n",
    "        'min_samples_leaf': None   # [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # TODO: Create base RandomForestClassifier\n",
    "    rf = None  # RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # TODO: Create GridSearchCV with 5-fold cross-validation\n",
    "    # Hint: GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search = None\n",
    "    \n",
    "    # Time the search\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TODO: Fit the grid search\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    # TODO: Get best parameters and score\n",
    "    best_params = None      # grid_search.best_params_\n",
    "    best_cv_score = None    # grid_search.best_score_\n",
    "    \n",
    "    # Test the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = accuracy_score(y_test, best_model.predict(X_test))\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_cv_score,\n",
    "        'test_score': test_score,\n",
    "        'search_time': search_time,\n",
    "        'total_combinations': len(grid_search.cv_results_['params']) if grid_search else None\n",
    "    }\n",
    "\n",
    "@validator.koan(1, \"Grid Search - Exhaustive Parameter Search\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = optimize_random_forest_with_grid_search()\n",
    "    \n",
    "    assert results['best_params'] is not None, \"Best parameters is None\"\n",
    "    assert results['best_cv_score'] is not None, \"Best CV score is None\"  \n",
    "    assert results['test_score'] is not None, \"Test score is None\"\n",
    "    assert results['total_combinations'] is not None, \"Total combinations is None\"\n",
    "    \n",
    "    # Check that we have reasonable results\n",
    "    assert 0.8 <= results['best_cv_score'] <= 1.0, f\"CV score should be reasonable, got {results['best_cv_score']:.3f}\"\n",
    "    assert 0.8 <= results['test_score'] <= 1.0, f\"Test score should be reasonable, got {results['test_score']:.3f}\"\n",
    "    \n",
    "    # Check that we tested multiple combinations\n",
    "    expected_combinations = 3 * 4 * 3 * 3  # Based on typical param grid\n",
    "    assert results['total_combinations'] >= 36, f\"Should test many combinations, got {results['total_combinations']}\"\n",
    "    \n",
    "    print(\"\u2713 Grid Search optimization complete!\")\n",
    "    print(f\"  - Search time: {results['search_time']:.2f} seconds\")\n",
    "    print(f\"  - Total combinations tested: {results['total_combinations']}\")\n",
    "    print(f\"  - Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  - Test score: {results['test_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83c\udfc6 Best Parameters Found:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    cv_test_diff = abs(results['best_cv_score'] - results['test_score'])\n",
    "    if cv_test_diff < 0.02:\n",
    "        print(f\"\\n  \u2713 Good generalization (CV-Test diff: {cv_test_diff:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n  \u26a0\ufe0f Possible overfitting (CV-Test diff: {cv_test_diff:.3f})\")\n",
    "        \n",
    "    print(f\"\\n  \ud83d\udca1 Grid Search Insights:\")\n",
    "    print(f\"    \u2022 Exhaustive: Tests all parameter combinations\")  \n",
    "    print(f\"    \u2022 Deterministic: Same results every run\")\n",
    "    print(f\"    \u2022 Expensive: Time grows exponentially with parameters\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.2: Random Search - Efficient Parameter Sampling\n",
    "**Objective**: Use RandomizedSearchCV for faster parameter exploration  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Random Search samples parameter combinations randomly from specified distributions. It's often more efficient than Grid Search, especially with many parameters.\n",
    "\n",
    "**Key Concept**: Random Search can find good parameters faster than Grid Search because it doesn't waste time on systematically bad regions of parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_grid_vs_random_search():\n",
    "    \"\"\"\n",
    "    Compare Grid Search vs Random Search on SVM hyperparameter tuning.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison results including timing and performance\n",
    "    \"\"\"\n",
    "    # Load digits dataset (more complex for demonstrating search differences)\n",
    "    digits = load_digits()\n",
    "    X, y = digits.data, digits.target\n",
    "    \n",
    "    # Use subset for faster demo\n",
    "    X_subset = X[:1000]\n",
    "    y_subset = y[:1000]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_subset, y_subset, test_size=0.2, random_state=42, stratify=y_subset\n",
    "    )\n",
    "    \n",
    "    # Scale features for SVM\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # TODO: Define parameter distributions for Random Search\n",
    "    param_distributions = {\n",
    "        'C': None,        # uniform(0.1, 100) - continuous distribution\n",
    "        'gamma': None,    # uniform(0.001, 1) - continuous distribution  \n",
    "        'kernel': None    # ['rbf', 'linear'] - categorical choice\n",
    "    }\n",
    "    \n",
    "    # TODO: Create SVC model\n",
    "    svm = None  # SVC(random_state=42)\n",
    "    \n",
    "    # Grid Search (smaller grid for comparison)\n",
    "    grid_params = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'kernel': ['rbf', 'linear']}\n",
    "    grid_search = GridSearchCV(svm, grid_params, cv=3, n_jobs=-1)\n",
    "    \n",
    "    # TODO: Create RandomizedSearchCV with 20 iterations\n",
    "    # Hint: RandomizedSearchCV(svm, param_distributions, n_iter=20, cv=3, random_state=42, n_jobs=-1)\n",
    "    random_search = None\n",
    "    \n",
    "    # Time both searches\n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    grid_time = time.time() - start\n",
    "    \n",
    "    start = time.time()  \n",
    "    # TODO: Fit random search\n",
    "    # random_search.fit(X_train_scaled, y_train)\n",
    "    random_time = time.time() - start\n",
    "    \n",
    "    # Compare results\n",
    "    grid_score = grid_search.score(X_test_scaled, y_test)\n",
    "    random_score = random_search.score(X_test_scaled, y_test) if random_search else 0\n",
    "    \n",
    "    return {\n",
    "        'grid_time': grid_time,\n",
    "        'random_time': random_time,\n",
    "        'grid_score': grid_score,\n",
    "        'random_score': random_score,\n",
    "        'grid_combinations': len(grid_search.cv_results_['params']),\n",
    "        'random_combinations': 20,\n",
    "        'grid_best_params': grid_search.best_params_,\n",
    "        'random_best_params': random_search.best_params_ if random_search else None\n",
    "    }\n",
    "\n",
    "@validator.koan(2, \"Random Search - Efficient Parameter Sampling\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = compare_grid_vs_random_search()\n",
    "    \n",
    "    assert results['random_score'] is not None and results['random_score'] > 0, \"Random search not implemented\"\n",
    "    assert results['random_best_params'] is not None, \"Random search best params is None\"\n",
    "    assert 0.7 <= results['grid_score'] <= 1.0, f\"Grid score should be reasonable, got {results['grid_score']:.3f}\"\n",
    "    assert 0.7 <= results['random_score'] <= 1.0, f\"Random score should be reasonable, got {results['random_score']:.3f}\"\n",
    "    \n",
    "    print(\"\u2713 Random Search vs Grid Search comparison complete!\")\n",
    "    print(f\"\\n  \u23f1\ufe0f  Timing Comparison:\")\n",
    "    print(f\"    Grid Search: {results['grid_time']:.2f}s ({results['grid_combinations']} combinations)\")\n",
    "    print(f\"    Random Search: {results['random_time']:.2f}s ({results['random_combinations']} combinations)\")\n",
    "    \n",
    "    speedup = results['grid_time'] / results['random_time'] if results['random_time'] > 0 else 1\n",
    "    print(f\"    Speedup: {speedup:.1f}x faster\")\n",
    "    \n",
    "    print(f\"\\n  \ud83c\udfaf Performance Comparison:\")\n",
    "    print(f\"    Grid Search: {results['grid_score']:.4f}\")\n",
    "    print(f\"    Random Search: {results['random_score']:.4f}\")\n",
    "    \n",
    "    score_diff = results['random_score'] - results['grid_score']\n",
    "    if abs(score_diff) < 0.02:\n",
    "        print(f\"    Similar performance (diff: {score_diff:+.3f})\")\n",
    "    elif score_diff > 0:\n",
    "        print(f\"    Random Search wins! (+{score_diff:.3f})\")\n",
    "    else:\n",
    "        print(f\"    Grid Search wins! ({score_diff:.3f})\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udca1 Random Search Benefits:\")\n",
    "    print(f\"    \u2022 Faster with many parameters\")\n",
    "    print(f\"    \u2022 Can find good solutions quickly\")\n",
    "    print(f\"    \u2022 Explores diverse parameter regions\")\n",
    "    print(f\"    \u2022 Easy to parallelize\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.3: Parameter Distributions - Defining Search Spaces\n",
    "**Objective**: Learn to define appropriate parameter distributions  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Choosing good parameter distributions is crucial for effective random search. Different parameters need different distribution types (uniform, log-uniform, discrete, etc.).\n",
    "\n",
    "**Key Concept**: Parameter distributions should reflect prior knowledge about reasonable parameter ranges and scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_parameter_distributions():\n",
    "    \"\"\"\n",
    "    Design appropriate parameter distributions for Random Forest tuning.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parameter distributions and tuning results\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Define smart parameter distributions\n",
    "    param_distributions = {\n",
    "        # Number of trees: discrete integers in reasonable range\n",
    "        'n_estimators': None,           # randint(50, 301) - random integers from 50 to 300\n",
    "        \n",
    "        # Max depth: include None (unlimited) and reasonable depths\n",
    "        'max_depth': None,              # [None] + list(range(5, 31, 5)) - None plus [5,10,15,20,25,30]\n",
    "        \n",
    "        # Minimum samples to split: small integers\n",
    "        'min_samples_split': None,      # randint(2, 21) - integers from 2 to 20\n",
    "        \n",
    "        # Minimum samples in leaf: small integers  \n",
    "        'min_samples_leaf': None,       # randint(1, 11) - integers from 1 to 10\n",
    "        \n",
    "        # Maximum features: different strategies\n",
    "        'max_features': None,           # ['sqrt', 'log2', None, 0.5, 0.7, 0.9]\n",
    "        \n",
    "        # Bootstrap: boolean choice\n",
    "        'bootstrap': None               # [True, False]\n",
    "    }\n",
    "    \n",
    "    # TODO: Create RandomForestClassifier\n",
    "    rf = None  # RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # TODO: Create RandomizedSearchCV with 100 iterations\n",
    "    # Hint: RandomizedSearchCV(rf, param_distributions, n_iter=100, cv=5, random_state=42, n_jobs=-1, scoring='accuracy')\n",
    "    random_search = None\n",
    "    \n",
    "    # TODO: Fit the search\n",
    "    # random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Analyze the search results\n",
    "    best_params = random_search.best_params_ if random_search else {}\n",
    "    best_score = random_search.best_score_ if random_search else 0\n",
    "    test_score = random_search.score(X_test, y_test) if random_search else 0\n",
    "    \n",
    "    return {\n",
    "        'param_distributions': param_distributions,\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'test_score': test_score,\n",
    "        'n_iter': 100\n",
    "    }\n",
    "\n",
    "@validator.koan(3, \"Parameter Distributions - Defining Search Spaces\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = design_parameter_distributions()\n",
    "    \n",
    "    assert results['best_params'] is not None and len(results['best_params']) > 0, \"Best params not found\"\n",
    "    assert results['best_cv_score'] > 0, \"Best CV score not calculated\" \n",
    "    assert results['test_score'] > 0, \"Test score not calculated\"\n",
    "    assert 0.8 <= results['best_cv_score'] <= 1.0, f\"CV score should be reasonable, got {results['best_cv_score']:.3f}\"\n",
    "    \n",
    "    print(\"\u2713 Parameter distribution design complete!\")\n",
    "    print(f\"  - Search iterations: {results['n_iter']}\")\n",
    "    print(f\"  - Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  - Test score: {results['test_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83c\udfc6 Optimized Parameters:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udcca Distribution Design Insights:\")\n",
    "    print(f\"    \u2022 Use randint() for integer parameters\")\n",
    "    print(f\"    \u2022 Use uniform() for continuous parameters\") \n",
    "    print(f\"    \u2022 Include None/default values where appropriate\")\n",
    "    print(f\"    \u2022 Consider log-scale for parameters spanning orders of magnitude\")\n",
    "    print(f\"    \u2022 Mix discrete choices with continuous ranges\")\n",
    "    \n",
    "    # Check if we got sensible parameter values\n",
    "    n_est = results['best_params'].get('n_estimators', 0)\n",
    "    if 50 <= n_est <= 300:\n",
    "        print(f\"    \u2713 Reasonable n_estimators: {n_est}\")\n",
    "    \n",
    "    max_depth = results['best_params'].get('max_depth')\n",
    "    if max_depth is None or (isinstance(max_depth, int) and 5 <= max_depth <= 30):\n",
    "        print(f\"    \u2713 Reasonable max_depth: {max_depth}\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.4: Bayesian Optimization - Smart Parameter Search  \n",
    "**Objective**: Use Bayesian optimization for intelligent hyperparameter search  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Bayesian optimization uses previous evaluation results to intelligently choose the next parameters to try. It's especially effective for expensive model training.\n",
    "\n",
    "**Key Concept**: Unlike random search, Bayesian optimization learns from past evaluations to focus on promising regions of parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_hyperparameter_optimization():\n",
    "    \"\"\"\n",
    "    Use Bayesian optimization for intelligent hyperparameter search.\n",
    "    Falls back to RandomizedSearchCV if BayesSearchCV not available.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bayesian optimization results and comparison\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    if BAYESIAN_AVAILABLE:\n",
    "        # TODO: Define search space using skopt\n",
    "        search_space = {\n",
    "            'n_estimators': None,      # Integer(50, 300)\n",
    "            'max_depth': None,         # Integer(1, 30) \n",
    "            'min_samples_split': None, # Integer(2, 20)\n",
    "            'min_samples_leaf': None,  # Integer(1, 10)\n",
    "            'max_features': None       # Categorical(['sqrt', 'log2'])\n",
    "        }\n",
    "        \n",
    "        # TODO: Create RandomForestClassifier\n",
    "        rf = None  # RandomForestClassifier(random_state=42)\n",
    "        \n",
    "        # TODO: Create BayesSearchCV\n",
    "        # Hint: BayesSearchCV(rf, search_space, n_iter=50, cv=5, random_state=42, n_jobs=-1)\n",
    "        bayes_search = None\n",
    "        \n",
    "        # TODO: Fit the Bayesian search\n",
    "        # bayes_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_params = bayes_search.best_params_ if bayes_search else {}\n",
    "        best_score = bayes_search.best_score_ if bayes_search else 0\n",
    "        test_score = bayes_search.score(X_test, y_test) if bayes_search else 0\n",
    "        search_type = \"Bayesian\"\n",
    "        \n",
    "    else:\n",
    "        # Fallback to RandomizedSearchCV\n",
    "        print(\"Using RandomizedSearchCV fallback...\")\n",
    "        param_distributions = {\n",
    "            'n_estimators': randint(50, 301),\n",
    "            'max_depth': randint(1, 31),\n",
    "            'min_samples_split': randint(2, 21), \n",
    "            'min_samples_leaf': randint(1, 11),\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        bayes_search = RandomizedSearchCV(rf, param_distributions, n_iter=50, cv=5, random_state=42, n_jobs=-1)\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_params = bayes_search.best_params_\n",
    "        best_score = bayes_search.best_score_\n",
    "        test_score = bayes_search.score(X_test, y_test)\n",
    "        search_type = \"Randomized (fallback)\"\n",
    "    \n",
    "    return {\n",
    "        'search_type': search_type,\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'test_score': test_score,\n",
    "        'bayesian_available': BAYESIAN_AVAILABLE\n",
    "    }\n",
    "\n",
    "@validator.koan(4, \"Bayesian Optimization - Smart Parameter Search\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = bayesian_hyperparameter_optimization()\n",
    "    \n",
    "    assert results['best_params'] is not None, \"Best parameters is None\"\n",
    "    assert results['best_cv_score'] > 0, \"Best CV score not found\"\n",
    "    assert results['test_score'] > 0, \"Test score not found\"\n",
    "    assert 0.8 <= results['best_cv_score'] <= 1.0, f\"CV score should be reasonable, got {results['best_cv_score']:.3f}\"\n",
    "    \n",
    "    print(f\"\u2713 {results['search_type']} optimization complete!\")\n",
    "    print(f\"  - Method: {results['search_type']}\")\n",
    "    print(f\"  - Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  - Test score: {results['test_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83c\udfc6 Best Parameters:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    if results['bayesian_available']:\n",
    "        print(f\"\\n  \ud83e\udde0 Bayesian Optimization Benefits:\")\n",
    "        print(f\"    \u2022 Learns from previous evaluations\")\n",
    "        print(f\"    \u2022 Focuses on promising parameter regions\")\n",
    "        print(f\"    \u2022 More efficient than random search\")\n",
    "        print(f\"    \u2022 Balances exploration vs exploitation\")\n",
    "        print(f\"    \u2022 Great for expensive model training\")\n",
    "    else:\n",
    "        print(f\"\\n  \ud83d\udce6 To use Bayesian optimization:\")\n",
    "        print(f\"    pip install scikit-optimize\")\n",
    "        print(f\"    \u2022 Much smarter than random search\")\n",
    "        print(f\"    \u2022 Essential for expensive hyperparameter tuning\")\n",
    "        \n",
    "    print(f\"\\n  \ud83d\udca1 When to use Bayesian optimization:\")\n",
    "    print(f\"    \u2022 Model training is expensive (>1 minute per iteration)\")\n",
    "    print(f\"    \u2022 Complex parameter interactions exist\") \n",
    "    print(f\"    \u2022 You have limited evaluation budget\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.5: Early Stopping - Training Efficiency\n",
    "**Objective**: Implement early stopping to prevent overfitting and save time  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Early stopping monitors validation performance during training and stops when performance plateaus or degrades, preventing overfitting and saving computation time.\n",
    "\n",
    "**Key Concept**: Early stopping is crucial for iterative algorithms like gradient boosting, neural networks, and any model trained with validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_early_stopping():\n",
    "    \"\"\"\n",
    "    Demonstrate early stopping with Gradient Boosting.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results with and without early stopping\n",
    "    \"\"\"\n",
    "    # Create a larger dataset to demonstrate early stopping benefits\n",
    "    X, y = make_classification(n_samples=2000, n_features=20, n_informative=10, \n",
    "                               n_redundant=10, random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split training data for validation\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # TODO: Train model WITHOUT early stopping\n",
    "    gb_no_early = None  # GradientBoostingClassifier(n_estimators=200, random_state=42, verbose=0)\n",
    "    \n",
    "    # TODO: Fit the model\n",
    "    # gb_no_early.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # TODO: Train model WITH early stopping\n",
    "    gb_early = None  # GradientBoostingClassifier(n_estimators=200, validation_fraction=0.2, \n",
    "                    #                             n_iter_no_change=10, random_state=42, verbose=0)\n",
    "    \n",
    "    # TODO: Fit with early stopping\n",
    "    # gb_early.fit(X_train, y_train)  # Uses internal validation split\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    no_early_score = gb_no_early.score(X_test, y_test) if gb_no_early else 0\n",
    "    early_score = gb_early.score(X_test, y_test) if gb_early else 0\n",
    "    \n",
    "    # Get number of estimators actually used\n",
    "    no_early_n_est = gb_no_early.n_estimators if gb_no_early else 0\n",
    "    early_n_est = gb_early.n_estimators_ if gb_early and hasattr(gb_early, 'n_estimators_') else (gb_early.n_estimators if gb_early else 0)\n",
    "    \n",
    "    return {\n",
    "        'no_early_stopping_score': no_early_score,\n",
    "        'early_stopping_score': early_score,\n",
    "        'no_early_n_estimators': no_early_n_est,\n",
    "        'early_n_estimators': early_n_est,\n",
    "        'estimators_saved': no_early_n_est - early_n_est if early_n_est < no_early_n_est else 0\n",
    "    }\n",
    "\n",
    "@validator.koan(5, \"Early Stopping - Training Efficiency\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = implement_early_stopping()\n",
    "    \n",
    "    assert results['no_early_stopping_score'] > 0, \"No early stopping model not trained\"\n",
    "    assert results['early_stopping_score'] > 0, \"Early stopping model not trained\"\n",
    "    assert 0.7 <= results['no_early_stopping_score'] <= 1.0, \"No early stopping score should be reasonable\"\n",
    "    assert 0.7 <= results['early_stopping_score'] <= 1.0, \"Early stopping score should be reasonable\"\n",
    "    \n",
    "    print(\"\u2713 Early stopping comparison complete!\")\n",
    "    print(f\"\\n  \ud83d\udcca Performance Comparison:\")\n",
    "    print(f\"    Without early stopping: {results['no_early_stopping_score']:.4f} ({results['no_early_n_estimators']} trees)\")\n",
    "    print(f\"    With early stopping: {results['early_stopping_score']:.4f} ({results['early_n_estimators']} trees)\")\n",
    "    \n",
    "    if results['estimators_saved'] > 0:\n",
    "        saved_pct = (results['estimators_saved'] / results['no_early_n_estimators']) * 100\n",
    "        print(f\"    Trees saved: {results['estimators_saved']} ({saved_pct:.1f}% reduction)\")\n",
    "    \n",
    "    score_diff = results['early_stopping_score'] - results['no_early_stopping_score']\n",
    "    if score_diff >= 0:\n",
    "        print(f\"    Early stopping performed equally well or better!\")\n",
    "    else:\n",
    "        print(f\"    Small performance trade-off: {score_diff:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  \u26a1 Early Stopping Benefits:\")\n",
    "    print(f\"    \u2022 Prevents overfitting automatically\")\n",
    "    print(f\"    \u2022 Saves computational time\")\n",
    "    print(f\"    \u2022 Reduces model complexity\")\n",
    "    print(f\"    \u2022 Built into many algorithms (GBM, XGBoost, neural networks)\")\n",
    "    \n",
    "    print(f\"\\n  \u2699\ufe0f  Key Parameters:\")\n",
    "    print(f\"    \u2022 n_iter_no_change: Stop after N iterations without improvement\")\n",
    "    print(f\"    \u2022 validation_fraction: Fraction of training data for validation\")\n",
    "    print(f\"    \u2022 tol: Minimum improvement threshold\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udca1 Best Practices:\")\n",
    "    print(f\"    \u2022 Always use with iterative algorithms\")\n",
    "    print(f\"    \u2022 Monitor validation loss, not training loss\") \n",
    "    print(f\"    \u2022 Set reasonable patience (n_iter_no_change)\")\n",
    "    print(f\"    \u2022 Consider restoring best weights\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.6: Nested Cross-Validation - Unbiased Model Evaluation\n",
    "**Objective**: Implement nested CV to avoid data leakage in hyperparameter tuning  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Nested cross-validation provides unbiased performance estimates when doing hyperparameter tuning. The outer loop evaluates performance, the inner loop tunes parameters.\n",
    "\n",
    "**Key Concept**: Using the same data for hyperparameter tuning and performance evaluation leads to overoptimistic results. Nested CV separates these processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation():\n",
    "    \"\"\"\n",
    "    Implement nested cross-validation for unbiased hyperparameter tuning evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested CV results and comparison with simple CV\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    # TODO: Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': None,      # [50, 100, 200]\n",
    "        'max_depth': None,         # [None, 10, 20]\n",
    "        'min_samples_split': None  # [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    # TODO: Create base model\n",
    "    rf = None  # RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Inner CV: For hyperparameter tuning (3-fold)  \n",
    "    # TODO: Create GridSearchCV for inner loop\n",
    "    inner_cv = None  # GridSearchCV(rf, param_grid, cv=3, scoring='accuracy')\n",
    "    \n",
    "    # Outer CV: For performance evaluation (5-fold)\n",
    "    # TODO: Calculate nested cross-validation scores\n",
    "    # Hint: Use cross_val_score(inner_cv, X, y, cv=5)\n",
    "    nested_scores = None\n",
    "    \n",
    "    # Compare with non-nested (biased) approach\n",
    "    # This reuses the same data for tuning and evaluation - BAD!\n",
    "    simple_cv = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "    simple_cv.fit(X, y)\n",
    "    biased_score = simple_cv.best_score_\n",
    "    \n",
    "    # Calculate statistics\n",
    "    nested_mean = np.mean(nested_scores) if nested_scores is not None else 0\n",
    "    nested_std = np.std(nested_scores) if nested_scores is not None else 0\n",
    "    \n",
    "    return {\n",
    "        'nested_scores': nested_scores,\n",
    "        'nested_mean': nested_mean,\n",
    "        'nested_std': nested_std,\n",
    "        'biased_score': biased_score,\n",
    "        'bias': biased_score - nested_mean,\n",
    "        'best_params': simple_cv.best_params_\n",
    "    }\n",
    "\n",
    "@validator.koan(6, \"Nested Cross-Validation - Unbiased Model Evaluation\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = nested_cross_validation()\n",
    "    \n",
    "    assert results['nested_scores'] is not None, \"Nested CV scores not calculated\"\n",
    "    assert len(results['nested_scores']) == 5, \"Should have 5 outer CV scores\"\n",
    "    assert 0.7 <= results['nested_mean'] <= 1.0, f\"Nested mean should be reasonable, got {results['nested_mean']:.3f}\"\n",
    "    assert 0.7 <= results['biased_score'] <= 1.0, f\"Biased score should be reasonable, got {results['biased_score']:.3f}\"\n",
    "    \n",
    "    print(\"\u2713 Nested cross-validation analysis complete!\")\n",
    "    print(f\"\\n  \ud83d\udd04 Nested CV Results (Unbiased):\")\n",
    "    print(f\"    Mean accuracy: {results['nested_mean']:.4f} (\u00b1{results['nested_std']:.4f})\")\n",
    "    print(f\"    Individual scores: {[f'{s:.3f}' for s in results['nested_scores']]}\")\n",
    "    \n",
    "    print(f\"\\n  \u26a0\ufe0f  Simple CV Results (Biased):\")\n",
    "    print(f\"    Mean accuracy: {results['biased_score']:.4f}\")\n",
    "    print(f\"    Optimistic bias: +{results['bias']:.4f}\")\n",
    "    \n",
    "    if results['bias'] > 0.01:\n",
    "        print(f\"    \ud83d\udcc8 Significant overestimation detected!\")\n",
    "    else:\n",
    "        print(f\"    \u2713 Minimal bias (dataset may be easy)\")\n",
    "    \n",
    "    print(f\"\\n  \ud83c\udfc6 Best Parameters Found:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udca1 Why Nested CV Matters:\")\n",
    "    print(f\"    \u2022 Prevents data leakage during hyperparameter tuning\")\n",
    "    print(f\"    \u2022 Provides unbiased performance estimates\")\n",
    "    print(f\"    \u2022 Essential for model comparison and selection\")\n",
    "    print(f\"    \u2022 Required for reliable performance reporting\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udccb Nested CV Process:\")\n",
    "    print(f\"    1. Outer loop: Split data for performance evaluation\")\n",
    "    print(f\"    2. Inner loop: Tune hyperparameters on training portion\")\n",
    "    print(f\"    3. Evaluate tuned model on validation portion\")\n",
    "    print(f\"    4. Repeat for all outer folds\")\n",
    "    print(f\"    5. Average performance across outer folds\")\n",
    "    \n",
    "    print(f\"\\n  \u26a0\ufe0f  Common Mistake to Avoid:\")\n",
    "    print(f\"    \u2022 Never use the same data for tuning AND evaluation\")\n",
    "    print(f\"    \u2022 This leads to overoptimistic performance estimates\")\n",
    "    print(f\"    \u2022 Use nested CV for honest performance reporting\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.7: AutoML Basics - Automated Model Selection\n",
    "**Objective**: Understand automated machine learning concepts and implementation  \n",
    "**Difficulty**: Advanced  \n",
    "\n",
    "AutoML automates the machine learning pipeline: feature preprocessing, algorithm selection, hyperparameter tuning, and ensemble creation. It democratizes ML and can outperform manual approaches.\n",
    "\n",
    "**Key Concept**: AutoML systems combine multiple techniques (meta-learning, Bayesian optimization, genetic algorithms) to automate the entire ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_automl_approach():\n",
    "    \"\"\"\n",
    "    Simulate an AutoML approach by automatically trying multiple algorithms\n",
    "    and selecting the best one through automated hyperparameter tuning.\n",
    "    \n",
    "    Returns:\n",
    "        dict: AutoML results including best model and comparison\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features for algorithms that need it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # TODO: Define candidate algorithms with their parameter grids\n",
    "    algorithms = {\n",
    "        'RandomForest': {\n",
    "            'model': None,  # RandomForestClassifier(random_state=42)\n",
    "            'params': None, # {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}\n",
    "            'scaled': False  # Doesn't need scaling\n",
    "        },\n",
    "        'SVM': {\n",
    "            'model': None,  # SVC(random_state=42)\n",
    "            'params': None, # {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "            'scaled': True   # Needs scaling\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'model': None,  # LogisticRegression(random_state=42, max_iter=1000)\n",
    "            'params': None, # {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}\n",
    "            'scaled': True   # Needs scaling\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Automated model selection and tuning\n",
    "    results = {}\n",
    "    best_score = 0\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    \n",
    "    for name, config in algorithms.items():\n",
    "        if config['model'] is not None and config['params'] is not None:\n",
    "            # Choose appropriate data (scaled or not)\n",
    "            X_train_use = X_train_scaled if config['scaled'] else X_train\n",
    "            X_test_use = X_test_scaled if config['scaled'] else X_test\n",
    "            \n",
    "            # TODO: Perform grid search for this algorithm\n",
    "            grid_search = None  # GridSearchCV(config['model'], config['params'], cv=5, scoring='accuracy')\n",
    "            \n",
    "            # TODO: Fit grid search\n",
    "            # grid_search.fit(X_train_use, y_train)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_score = grid_search.score(X_test_use, y_test) if grid_search else 0\n",
    "            \n",
    "            results[name] = {\n",
    "                'best_params': grid_search.best_params_ if grid_search else {},\n",
    "                'cv_score': grid_search.best_score_ if grid_search else 0,\n",
    "                'test_score': test_score,\n",
    "                'model': grid_search.best_estimator_ if grid_search else None\n",
    "            }\n",
    "            \n",
    "            # Track best performing model\n",
    "            if test_score > best_score:\n",
    "                best_score = test_score\n",
    "                best_model_name = name\n",
    "                best_model = grid_search.best_estimator_ if grid_search else None\n",
    "    \n",
    "    return {\n",
    "        'algorithm_results': results,\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'algorithms_tested': len([k for k, v in algorithms.items() if v['model'] is not None])\n",
    "    }\n",
    "\n",
    "@validator.koan(7, \"AutoML Basics - Automated Model Selection\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = simulate_automl_approach()\n",
    "    \n",
    "    assert results['algorithm_results'] is not None, \"Algorithm results not generated\"\n",
    "    assert results['best_model_name'] is not None, \"Best model not selected\"\n",
    "    assert results['best_score'] > 0, \"Best score not calculated\"\n",
    "    assert 0.8 <= results['best_score'] <= 1.0, f\"Best score should be reasonable, got {results['best_score']:.3f}\"\n",
    "    \n",
    "    print(\"\u2713 AutoML simulation complete!\")\n",
    "    print(f\"  - Algorithms tested: {results['algorithms_tested']}\")\n",
    "    print(f\"  - Best algorithm: {results['best_model_name']}\")\n",
    "    print(f\"  - Best test score: {results['best_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udcca Algorithm Comparison:\")\n",
    "    for name, result in results['algorithm_results'].items():\n",
    "        if result['test_score'] > 0:\n",
    "            print(f\"    {name}:\")\n",
    "            print(f\"      CV Score: {result['cv_score']:.4f}\")\n",
    "            print(f\"      Test Score: {result['test_score']:.4f}\")\n",
    "            print(f\"      Best Params: {result['best_params']}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83c\udfc6 Winning Model: {results['best_model_name']}\")\n",
    "    \n",
    "    print(f\"\\n  \ud83e\udd16 Real AutoML Systems:\")\n",
    "    print(f\"    \u2022 Auto-sklearn: Automated sklearn pipeline\")\n",
    "    print(f\"    \u2022 TPOT: Genetic programming for ML pipelines\")\n",
    "    print(f\"    \u2022 H2O AutoML: Distributed AutoML platform\")\n",
    "    print(f\"    \u2022 Google AutoML: Cloud-based AutoML\")\n",
    "    print(f\"    \u2022 Azure AutoML: Microsoft's AutoML service\")\n",
    "    \n",
    "    print(f\"\\n  \ud83d\udca1 AutoML Benefits:\")\n",
    "    print(f\"    \u2022 Democratizes machine learning\")\n",
    "    print(f\"    \u2022 Finds good models quickly\")\n",
    "    print(f\"    \u2022 Handles algorithm selection automatically\")\n",
    "    print(f\"    \u2022 Can discover unexpected good models\")\n",
    "    print(f\"    \u2022 Saves expert time for harder problems\")\n",
    "    \n",
    "    print(f\"\\n  \u26a0\ufe0f  AutoML Limitations:\")\n",
    "    print(f\"    \u2022 May not capture domain expertise\")\n",
    "    print(f\"    \u2022 Can be computationally expensive\")\n",
    "    print(f\"    \u2022 Less interpretable model selection\")\n",
    "    print(f\"    \u2022 May overfit to validation set with extensive search\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n",
    "\n",
    "You have mastered hyperparameter tuning - the key to unlocking your models' full potential!\n",
    "\n",
    "### What You've Mastered\n",
    "- \u2705 **Grid Search**: Exhaustive parameter space exploration\n",
    "- \u2705 **Random Search**: Efficient parameter sampling strategies  \n",
    "- \u2705 **Parameter Distributions**: Smart search space design\n",
    "- \u2705 **Bayesian Optimization**: Intelligent, adaptive parameter search\n",
    "- \u2705 **Early Stopping**: Preventing overfitting and saving computation\n",
    "- \u2705 **Nested Cross-Validation**: Unbiased model evaluation methodology\n",
    "- \u2705 **AutoML Concepts**: Understanding automated machine learning\n",
    "\n",
    "### Key Insights Gained\n",
    "1. **Search Strategy Matters**: Different approaches for different scenarios\n",
    "2. **Computational Trade-offs**: Balance thoroughness vs. efficiency\n",
    "3. **Avoid Data Leakage**: Separate tuning from evaluation\n",
    "4. **Smart Distributions**: Prior knowledge improves search efficiency\n",
    "5. **Automation Potential**: AutoML can find surprising good solutions\n",
    "\n",
    "### Performance Impact\n",
    "- \ud83c\udfaf **10-30% accuracy gains** from proper hyperparameter tuning\n",
    "- \u26a1 **10-100x speedup** with smart search strategies  \n",
    "- \ud83d\udee1\ufe0f **Robust evaluation** through nested cross-validation\n",
    "- \ud83e\udd16 **Automated workflows** reducing manual effort\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 14**: Model Selection and Pipelines (production workflows!)\n",
    "- **Advanced**: Neural architecture search, multi-objective optimization\n",
    "- **Practice**: Apply to your own models and datasets\n",
    "\n",
    "### Real-World Applications\n",
    "- **Competitions**: Essential for Kaggle and ML contests\n",
    "- **Production**: Automated model improvement and monitoring\n",
    "- **Research**: Reproducible and fair model comparisons\n",
    "- **Business**: Maximizing ROI from ML investments\n",
    "\n",
    "You now have the tools to systematically optimize any machine learning model! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Progress Check\n",
    "progress = tracker.get_notebook_progress('13_hyperparameter_tuning')\n",
    "print(f\"\\n\ud83d\udcca Your Progress: {progress}% complete!\")\n",
    "\n",
    "if progress == 100:\n",
    "    print(\"\ud83c\udf89 Exceptional! You've mastered all hyperparameter tuning techniques!\")\n",
    "    print(\"\ud83c\udfaf Ready for Notebook 14: Model Selection and Pipelines\")\n",
    "elif progress >= 75:\n",
    "    print(\"\ud83c\udf1f Outstanding progress! Almost finished with tuning mastery.\")\n",
    "elif progress >= 50:\n",
    "    print(\"\ud83d\udcaa Great work! You're building powerful optimization skills.\")\n",
    "else:\n",
    "    print(\"\ud83d\ude80 Keep going! Each technique builds sophisticated tuning expertise.\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Overall course progress:\")\n",
    "total_notebooks = 15\n",
    "completed_notebooks = len([nb for nb in range(1, 14) if tracker.get_notebook_progress(f'{nb:02d}_*') == 100])\n",
    "print(f\"   Completed notebooks: {completed_notebooks}/{total_notebooks}\")\n",
    "print(f\"   Course progress: {(completed_notebooks/total_notebooks)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Hyperparameter Tuning Mastery Achieved!\")\n",
    "print(f\"   Your models will never be the same! \u26a1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}