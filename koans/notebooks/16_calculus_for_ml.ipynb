{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus for Machine Learning - Data Science Koans\n",
    "\n",
    "Welcome to Notebook 16: Calculus for Machine Learning! Mastering derivatives, gradients, and Hessians gives you the intuition needed to optimize models and reason about loss landscapes.\n",
    "\n",
    "## What You Will Learn\n",
    "- Slope, tangents, and limit-based derivatives\n",
    "- Differentiability and rulebook shortcuts\n",
    "- Critical points, gradient descent, and higher-order derivatives\n",
    "- Partial derivatives, gradients, Jacobians, and Hessians\n",
    "\n",
    "## Prerequisites\n",
    "- NumPy fundamentals and linear algebra KOANs 1.11-1.24 (Notebook 01)\n",
    "- Familiarity with vectorized computations from Pandas Essentials (Notebook 02)\n",
    "\n",
    "## How to Use\n",
    "1. Run the setup cell first.\n",
    "2. Read the concept summary for each koan.\n",
    "3. Complete the `TODO` blocks.\n",
    "4. Execute the validation cell to receive feedback.\n",
    "5. Iterate until every koan reports success\u2014calculus intuition unlocked!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run first!\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import math\n",
    "import numpy as np\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "\n",
    "validator = KoanValidator(\"16_calculus_for_ml\")\n",
    "tracker = ProgressTracker()\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Progress: {tracker.get_notebook_progress('16_calculus_for_ml')}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional visualization helpers\n",
    "The cell below builds small helper functions used in optional demo plots throughout the notebook. Feel free to run and explore, but no TODOs here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional plotting utilities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_tangent(f, a, derivative):\n",
    "    '''Plot function f and tangent line at point a.'''\n",
    "    xs = np.linspace(a - 3, a + 3, 200)\n",
    "    ys = f(xs)\n",
    "    slope = derivative(a)\n",
    "    intercept = f(a) - slope * a\n",
    "    tangent = slope * xs + intercept\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(xs, ys, label='f(x)')\n",
    "    plt.plot(xs, tangent, '--', label='tangent at x=a')\n",
    "    plt.scatter([a], [f(a)], color='red')\n",
    "    plt.title('Function and Tangent')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_descent_path(path, f):\n",
    "    '''Visualize 1D gradient descent path.'''\n",
    "    xs = np.linspace(min(path) - 1, max(path) + 1, 200)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(xs, f(xs), label='f(x)')\n",
    "    plt.plot(path, f(np.array(path)), 'o--', label='iterates')\n",
    "    plt.title('Gradient Descent Trajectory')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.1: Secant Slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Compute the slope between two points on a curve\n",
    "**Difficulty**: Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant_slope(p1, p2):\n",
    "    '''Return the slope of the line through points p1 and p2.'''\n",
    "    # TODO: Implement the slope formula (rise over run).\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(1, \"Secant Slope\", difficulty=\"Beginner\")\n",
    "def validate():\n",
    "    slope = secant_slope((1.0, 3.0), (4.0, 15.0))\n",
    "    assert np.isclose(slope, 4.0), \"Use (y2 - y1) / (x2 - x1).\"\n",
    "    slope2 = secant_slope((-2.0, 5.0), (1.0, -4.0))\n",
    "    assert np.isclose(slope2, -3.0), \"Handle negative slopes correctly.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.2: Constant Slope for Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Confirm that linear functions have constant slope\n",
    "**Difficulty**: Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_slope(m, b):\n",
    "    '''Return the slope of f(x) = m*x + b.'''\n",
    "    # TODO: Compute the derivative analytically and return the slope.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(2, \"Constant Slope\", difficulty=\"Beginner\")\n",
    "def validate():\n",
    "    assert np.isclose(line_slope(5.0, -3.0), 5.0), \"Slope should equal m regardless of b.\"\n",
    "    assert np.isclose(line_slope(-1.5, 10.0), -1.5), \"Slope must match the linear coefficient.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.3: Numeric Derivative of $x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Approximate derivatives using finite differences\n",
    "**Difficulty**: Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_derivative_x2(x, h=1e-5):\n",
    "    '''Return the numeric derivative of f(x)=x**2 at point x using central differences.'''\n",
    "    # TODO: Implement the symmetric difference quotient.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(3, \"Numeric Derivative\", difficulty=\"Beginner\")\n",
    "def validate():\n",
    "    for point in [-2.0, -0.5, 0.0, 1.0, 3.0]:\n",
    "        approx = numeric_derivative_x2(point)\n",
    "        expected = 2 * point\n",
    "        assert np.isclose(approx, expected, atol=1e-4), \"Central difference should approximate 2x.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.4: Tangent Line to $x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Derive the tangent line equation at a point\n",
    "**Difficulty**: Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent_line_x2(a):\n",
    "    '''Return (slope, intercept) for the tangent to x**2 at x=a.'''\n",
    "    # TODO: Use derivative 2a and point-slope form to find the line.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(4, \"Tangent Line\", difficulty=\"Beginner\")\n",
    "def validate():\n",
    "    slope, intercept = tangent_line_x2(2.0)\n",
    "    assert np.isclose(slope, 4.0), \"Derivative of x^2 at x=2 is 4.\"\n",
    "    assert np.isclose(intercept, -0.0), \"Line should pass through (2, 4).\"\n",
    "    slope0, intercept0 = tangent_line_x2(0.0)\n",
    "    assert np.isclose(slope0, 0.0), \"Derivative at zero is zero.\"\n",
    "    assert np.isclose(intercept0, 0.0), \"Tangent at origin is y=0.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.5: Differentiability Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Compare one-sided derivatives to test differentiability\n",
    "**Difficulty**: Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_derivative_summary(h=1e-5):\n",
    "    '''Return left derivative, right derivative, and differentiability flag for |x| at x=0.'''\n",
    "    # TODO: Approximate derivatives from both sides and decide if they match.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(5, \"Differentiability\", difficulty=\"Beginner\")\n",
    "def validate():\n",
    "    summary = abs_derivative_summary()\n",
    "    assert isinstance(summary, dict), \"Return a dictionary with keys left, right, differentiable.\"\n",
    "    left = summary.get('left')\n",
    "    right = summary.get('right')\n",
    "    diff = summary.get('differentiable')\n",
    "    assert np.isclose(left, -1.0, atol=1e-4), \"Left derivative of |x| at 0 is -1.\"\n",
    "    assert np.isclose(right, 1.0, atol=1e-4), \"Right derivative of |x| at 0 is 1.\"\n",
    "    assert diff is False, \"Absolute value is not differentiable at 0.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.6: Power Rule Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Build derivative functions for power functions\n",
    "**Difficulty**: Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_rule_derivative(exponent):\n",
    "    '''Return a function computing the derivative of f(x)=x**exponent.'''\n",
    "    # TODO: Implement the power rule for positive integer exponents.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(6, \"Power Rule\", difficulty=\"Beginner\")\n",
    "def validate():\n",
    "    derivative = power_rule_derivative(5)\n",
    "    assert callable(derivative), \"Return a callable derivative function.\"\n",
    "    for x in [-2.0, -0.5, 1.0, 2.0]:\n",
    "        assert np.isclose(derivative(x), 5 * (x ** 4)), \"Derivative of x^5 is 5x^4.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.7: Constant Function Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Practice derivative basics\n",
    "**Difficulty**: Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_derivative(value):\n",
    "    '''Return a function evaluating the derivative of f(x)=value.'''\n",
    "    # TODO: Build a callable that always returns zero regardless of x.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(7, \"Constant Derivative\", difficulty=\"Beginner\")\n",
    "def validate():\n",
    "    derivative = constant_derivative(42)\n",
    "    assert callable(derivative), \"Return a callable.\"\n",
    "    xs = np.linspace(-5, 5, 5)\n",
    "    assert np.allclose([derivative(x) for x in xs], 0.0), \"Derivative of a constant is zero.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.8: Product Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Combine derivatives using the product rule\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_rule(g, h, g_prime, h_prime):\n",
    "    '''Return a derivative function for f(x)=g(x)*h(x).'''\n",
    "    # TODO: Implement the product rule using provided derivative functions.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(8, \"Product Rule\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    g = lambda x: x ** 2\n",
    "    h = lambda x: np.sin(x)\n",
    "    g_prime = lambda x: 2 * x\n",
    "    h_prime = lambda x: np.cos(x)\n",
    "    derivative = product_rule(g, h, g_prime, h_prime)\n",
    "    assert callable(derivative), \"Return a callable derivative.\"\n",
    "    for x in [-1.0, 0.0, 1.0, 2.0]:\n",
    "        expected = g_prime(x) * h(x) + g(x) * h_prime(x)\n",
    "        assert np.isclose(derivative(x), expected), \"Apply the product rule exactly.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.9: Quotient Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Differentiate ratios of functions\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotient_rule(g, h, g_prime, h_prime):\n",
    "    '''Return a derivative function for f(x)=g(x)/h(x).'''\n",
    "    # TODO: Implement the quotient rule safely.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(9, \"Quotient Rule\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    g = lambda x: x ** 2 + 1\n",
    "    h = lambda x: x + 2\n",
    "    g_prime = lambda x: 2 * x\n",
    "    h_prime = lambda x: 1\n",
    "    derivative = quotient_rule(g, h, g_prime, h_prime)\n",
    "    for x in [-1.0, 0.0, 1.0, 2.0]:\n",
    "        expected = (g_prime(x) * h(x) - g(x) * h_prime(x)) / (h(x) ** 2)\n",
    "        assert np.isclose(derivative(x), expected), \"Apply the quotient rule correctly.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.10: Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Differentiate composite functions\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_rule(outer_prime, inner, inner_prime):\n",
    "    '''Return derivative function for f(x)=outer(inner(x)).'''\n",
    "    # TODO: Use the chain rule to combine provided components.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(10, \"Chain Rule\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    inner = lambda x: x ** 3 + 1\n",
    "    inner_prime = lambda x: 3 * (x ** 2)\n",
    "    outer_prime = lambda z: 2 * z\n",
    "    derivative = chain_rule(outer_prime, inner, inner_prime)\n",
    "    for x in [-2.0, -1.0, 0.5, 1.5]:\n",
    "        expected = outer_prime(inner(x)) * inner_prime(x)\n",
    "        assert np.isclose(derivative(x), expected), \"Chain rule should multiply outer'(inner(x)) by inner'(x).\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.11: Exponential and Logarithmic Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Reinforce special derivatives\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_log_derivatives(x):\n",
    "    '''Return a tuple (d_exp, d_log) for derivatives of e**x and ln(x).'''\n",
    "    # TODO: Compute derivatives analytically.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(11, \"Exp & Log Derivatives\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    for x in [0.5, 1.0, 2.0]:\n",
    "        d_exp, d_log = exp_log_derivatives(x)\n",
    "        assert np.isclose(d_exp, math.exp(x)), \"Derivative of e^x is e^x.\"\n",
    "        assert np.isclose(d_log, 1 / x), \"Derivative of ln(x) is 1/x.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.12: Trigonometric Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Differentiate sine, cosine, and tangent\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trig_derivatives(x):\n",
    "    '''Return a dict with derivatives of sin, cos, and tan at x.'''\n",
    "    # TODO: Fill in the standard trigonometric derivatives.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(12, \"Trig Derivatives\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    for x in [0.0, math.pi / 6, math.pi / 3]:\n",
    "        result = trig_derivatives(x)\n",
    "        assert np.isclose(result['sin'], math.cos(x)), \"d/dx sin(x) = cos(x).\"\n",
    "        assert np.isclose(result['cos'], -math.sin(x)), \"d/dx cos(x) = -sin(x).\"\n",
    "        assert np.isclose(result['tan'], 1 / (math.cos(x) ** 2)), \"d/dx tan(x) = sec^2(x).\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.13: Critical Points of a Cubic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Solve derivative=0 for a quadratic derivative\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critical_points_quadratic(a, b, c):\n",
    "    '''Return sorted real roots of a*x**2 + b*x + c = 0.'''\n",
    "    # TODO: Solve the quadratic equation and return real solutions only.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(13, \"Critical Points\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    roots = critical_points_quadratic(3.0, -6.0, 0.0)\n",
    "    assert np.allclose(roots, [0.0, 2.0]), \"Derivative 3x^2 - 6x = 0 has roots at 0 and 2.\"\n",
    "    roots_single = critical_points_quadratic(1.0, 2.0, 1.0)\n",
    "    assert np.allclose(roots_single, [-1.0]), \"Handle repeated roots gracefully.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.14: Second Derivative Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Classify stationary points using the second derivative\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_points(second_derivative, points):\n",
    "    '''Return dict mapping point -> classification ('min', 'max', 'saddle').'''\n",
    "    # TODO: Use the sign of the second derivative to classify each point.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(14, \"Second Derivative Test\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    f_second = lambda x: 6 * x - 6  # second derivative of x^3 - 3x^2 + 2\n",
    "    classification = classify_points(f_second, [0.0, 2.0])\n",
    "    assert classification[0.0] == 'max', \"Negative second derivative indicates local maximum.\"\n",
    "    assert classification[2.0] == 'min', \"Positive second derivative indicates local minimum.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.15: Gradient Descent Step (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Implement a single gradient descent update in one dimension\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step_1d(x, grad, learning_rate):\n",
    "    '''Return the updated point after one gradient descent step.'''\n",
    "    # TODO: Move opposite the gradient scaled by learning_rate.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(15, \"Gradient Descent 1D\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    grad = lambda x: 2 * x  # derivative of x^2\n",
    "    next_x = gradient_descent_step_1d(3.0, grad, 0.1)\n",
    "    assert np.isclose(next_x, 3.0 - 0.1 * 6.0), \"Apply x - lr * grad(x).\"\n",
    "    next_x2 = gradient_descent_step_1d(-2.0, grad, 0.2)\n",
    "    assert np.isclose(next_x2, -2.0 - 0.2 * (-4.0)), \"Handle negative points correctly.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.16: Higher-Order Derivative of Power Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Compute the k-th derivative of x^n\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kth_derivative_power(exponent, order):\n",
    "    '''Return a function for the k-th derivative of f(x)=x**exponent.'''\n",
    "    # TODO: Implement factorial-like coefficient for successive derivatives.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(16, \"Higher-Order Power Derivative\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    derivative = kth_derivative_power(5, 2)  # second derivative of x^5 -> 20x^3\n",
    "    for x in [0.0, 1.0, 2.0]:\n",
    "        expected = 5 * 4 * (x ** 3)\n",
    "        assert np.isclose(derivative(x), expected), \"Apply descending products for each derivative order.\"\n",
    "    zero_derivative = kth_derivative_power(3, 5)\n",
    "    assert np.isclose(zero_derivative(2.0), 0.0), \"Derivative beyond exponent should be zero.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.17: Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Approximate partial derivatives numerically\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivatives(func, point, h=1e-5):\n",
    "    '''Return (df/dx, df/dy) at the given point using central differences.'''\n",
    "    # TODO: Compute partial derivatives numerically.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(17, \"Partial Derivatives\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    func = lambda x, y: x ** 2 * y + np.exp(x * y)\n",
    "    df_dx, df_dy = partial_derivatives(func, (1.0, 2.0))\n",
    "    expected_dx = 2 * 1.0 * 2.0 + 2.0 * np.exp(2.0)\n",
    "    expected_dy = (1.0 ** 2) + 1.0 * np.exp(2.0)\n",
    "    assert np.isclose(df_dx, expected_dx, atol=1e-3), \"Approximate df/dx accurately.\"\n",
    "    assert np.isclose(df_dy, expected_dy, atol=1e-3), \"Approximate df/dy accurately.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.18: Gradient Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Assemble gradient vectors for multivariate functions\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(func, point, h=1e-5):\n",
    "    '''Return gradient vector as NumPy array for func: R^n -> R.'''\n",
    "    # TODO: Use finite differences to approximate each partial derivative.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(18, \"Gradient Vector\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    func = lambda x, y: (x - 1) ** 2 + 2 * (y + 2) ** 2\n",
    "    grad = gradient(func, (2.0, -2.0))\n",
    "    assert isinstance(grad, np.ndarray), \"Return NumPy array.\"\n",
    "    assert grad.shape == (2,), \"Gradient should have same dimension as input.\"\n",
    "    expected = np.array([2 * (2.0 - 1.0), 4 * (-2.0 + 2.0)])\n",
    "    assert np.allclose(grad, expected, atol=1e-3), \"Compute gradient components correctly.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.19: Gradient Descent Step (2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Implement gradient descent in multiple dimensions\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step_nd(point, grad_func, learning_rate):\n",
    "    '''Return updated NumPy array after one gradient descent step.'''\n",
    "    # TODO: Move opposite the gradient direction.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(19, \"Gradient Descent 2D\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    grad = lambda x, y: np.array([2 * (x - 1), 4 * (y + 2)])\n",
    "    updated = gradient_descent_step_nd(np.array([2.0, -2.0]), grad, 0.1)\n",
    "    expected = np.array([2.0, -2.0]) - 0.1 * grad(2.0, -2.0)\n",
    "    assert np.allclose(updated, expected), \"Apply x - lr * grad(x).\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.20: Jacobian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Approximate Jacobians for vector-valued functions\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(functions, point, h=1e-5):\n",
    "    '''Return the Jacobian matrix evaluated at point for list of functions.'''\n",
    "    # TODO: Use finite differences for each output component.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(20, \"Jacobian\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    funcs = [\n",
    "        lambda x, y: x ** 2 * y,\n",
    "        lambda x, y: np.sin(x) + np.cos(y)\n",
    "    ]\n",
    "    J = jacobian(funcs, (1.0, 0.5))\n",
    "    assert isinstance(J, np.ndarray) and J.shape == (2, 2), \"Jacobian should be 2x2.\"\n",
    "    expected = np.array([\n",
    "        [2 * 1.0 * 0.5, 1.0 ** 2],\n",
    "        [np.cos(1.0), -np.sin(0.5)]\n",
    "    ])\n",
    "    assert np.allclose(J, expected, atol=1e-3), \"Approximate partial derivatives correctly.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.21: Hessian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Compute second-order partial derivatives\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(func, point, h=1e-4):\n",
    "    '''Return Hessian matrix of scalar function at given point.'''\n",
    "    # TODO: Estimate second derivatives using finite differences.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(21, \"Hessian\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    func = lambda x, y: x ** 2 + x * y + 2 * y ** 2\n",
    "    H = hessian(func, (1.0, -1.0))\n",
    "    assert isinstance(H, np.ndarray) and H.shape == (2, 2), \"Return 2x2 Hessian.\"\n",
    "    expected = np.array([\n",
    "        [2.0, 1.0],\n",
    "        [1.0, 4.0]\n",
    "    ])\n",
    "    assert np.allclose(H, expected, atol=1e-2), \"Compute second derivatives accurately.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 16.22: Classify Stationary Point with Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Interpret Hessian eigenvalues\n",
    "**Difficulty**: Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_stationary_point(hessian_matrix, tol=1e-8):\n",
    "    '''Return 'minimum', 'maximum', or 'saddle' based on Hessian eigenvalues.'''\n",
    "    # TODO: Inspect eigenvalues to classify the stationary point.\n",
    "    pass\n",
    "\n",
    "\n",
    "@validator.koan(22, \"Hessian Classification\", difficulty=\"Intermediate\")\n",
    "def validate():\n",
    "    H_min = np.array([[2.0, 0.0], [0.0, 5.0]])\n",
    "    H_max = np.array([[-3.0, 0.0], [0.0, -1.0]])\n",
    "    H_saddle = np.array([[2.0, 1.0], [1.0, -3.0]])\n",
    "    assert classify_stationary_point(H_min) == 'minimum', \"Positive-definite Hessian -> minimum.\"\n",
    "    assert classify_stationary_point(H_max) == 'maximum', \"Negative-definite Hessian -> maximum.\"\n",
    "    assert classify_stationary_point(H_saddle) == 'saddle', \"Mixed signs -> saddle.\"\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You completed Calculus for Machine Learning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = tracker.get_notebook_progress('16_calculus_for_ml')\n",
    "print(f\"Final Progress: {progress}%\")\n",
    "if progress == 100:\n",
    "    print(\"Calculus concepts unlocked! \ud83c\udf89\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}